{
  "name": "QA General",
  "description": "General QA and testing questions",
  "questions": [
    {
      "id": 1,
      "question": "What is the difference between functional and non-functional testing?",
      "answer": "FUNCTIONAL: Tests what the system does (features, functionality). Examples: login, checkout, search. NON-FUNCTIONAL: Tests how the system performs (performance, security, usability). Examples: load testing, security audits, accessibility.",
      "options": [
        "Functional tests features, non-functional tests performance",
        "They test the same thing",
        "Functional is automated, non-functional is manual"
      ],
      "correctOption": 0,
      "difficulty": "easy",
      "tags": [
        "testing-types"
      ]
    },
    {
      "id": 2,
      "question": "Explain the testing pyramid concept.",
      "answer": "Testing pyramid shows test distribution: BASE (most tests) - Unit tests (fast, isolated, many). MIDDLE - Integration tests (moderate speed, test interactions). TOP (fewest) - E2E tests (slow, expensive, full user flows). Principle: more lower-level tests, fewer high-level tests.",
      "options": [
        "Equal distribution of all test types",
        "More unit tests at base, fewer E2E at top",
        "Only E2E tests are needed"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "strategy",
        "best-practices"
      ]
    },
    {
      "id": 3,
      "question": "What is test coverage and is 100% coverage a good goal?",
      "answer": "Test coverage measures % of code executed by tests. 100% coverage â‰  bug-free code. Problems: 1) Can test wrong things, 2) Doesn't guarantee quality assertions, 3) Expensive to maintain. Better: focus on critical paths, aim for 70-80% coverage with meaningful tests.",
      "options": [
        "100% coverage guarantees no bugs",
        "100% coverage doesn't guarantee quality, aim for 70-80%",
        "Coverage metrics are useless"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "metrics",
        "strategy"
      ]
    },
    {
      "id": 4,
      "question": "What are test doubles? Name and explain the main types.",
      "answer": "Test doubles are fake objects used in testing. Types: 1) Dummy - passed but never used, 2) Stub - returns predefined data, 3) Spy - records how it was called, 4) Mock - verifies behavior/calls, 5) Fake - working implementation (simpler). Use to isolate code under test from dependencies.",
      "options": [
        "Backup copies of tests",
        "Fake objects like stubs, mocks, spies",
        "Duplicate test cases for redundancy"
      ],
      "correctOption": 1,
      "difficulty": "hard",
      "tags": [
        "mocking",
        "test-doubles"
      ]
    },
    {
      "id": 5,
      "question": "What is the difference between black box and white box testing?",
      "answer": "BLACK BOX: Tests functionality without knowing internal structure. Focus on inputs/outputs. Used for integration/E2E tests. WHITE BOX: Tests internal logic, code structure. Requires code knowledge. Used for unit tests. Grey box combines both approaches.",
      "options": [
        "Black box tests internals, white box tests UI",
        "Black box tests without code knowledge, white box requires it",
        "Both require complete code access"
      ],
      "correctOption": 1,
      "difficulty": "easy",
      "tags": [
        "testing-types",
        "approaches"
      ]
    },
    {
      "id": 6,
      "question": "Explain regression testing and when it's necessary.",
      "answer": "Regression testing verifies existing functionality still works after changes. When needed: 1) After bug fixes, 2) New features added, 3) Code refactoring, 4) Environment changes. Use automated tests for efficiency. Prevents breaking existing features while developing new ones.",
      "options": [
        "Testing that goes backwards in time",
        "Verifying existing features after changes",
        "Only testing new features"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "regression",
        "automation"
      ]
    },
    {
      "id": 7,
      "question": "What is the AAA pattern in testing?",
      "answer": "AAA stands for Arrange-Act-Assert. Structure: 1) ARRANGE: Set up test data and preconditions, 2) ACT: Execute the code under test, 3) ASSERT: Verify the expected outcome. Makes tests readable and maintainable. Example: arrange user data, act by calling login(), assert user is logged in.",
      "options": [
        "Always Automate All tests",
        "Arrange-Act-Assert test structure",
        "Advanced Automation Architecture"
      ],
      "correctOption": 1,
      "difficulty": "easy",
      "tags": [
        "patterns",
        "best-practices"
      ]
    },
    {
      "id": 8,
      "question": "What is test-driven development (TDD)?",
      "answer": "TDD: Write tests before code. Cycle: 1) Write failing test (RED), 2) Write minimal code to pass (GREEN), 3) Refactor (REFACTOR). Benefits: Better design, higher coverage, fewer bugs. Challenges: Slower initially, requires discipline. Ensures code is testable from the start.",
      "options": [
        "Testing during development",
        "Write tests before code (Red-Green-Refactor)",
        "Testing done by developers only"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "TDD",
        "methodologies"
      ]
    },
    {
      "id": 9,
      "question": "What are flaky tests and how do you handle them?",
      "answer": "Flaky tests: Pass/fail inconsistently without code changes. Causes: 1) Timing issues, 2) External dependencies, 3) Test order dependency, 4) Random data. Solutions: Fix root cause, add retries (temporary), use proper waits, mock dependencies, ensure test isolation. Never ignore - they erode trust in test suite.",
      "options": [
        "Tests written in Flake framework",
        "Tests that pass/fail inconsistently",
        "Tests with poor code quality"
      ],
      "correctOption": 1,
      "difficulty": "hard",
      "tags": [
        "flaky-tests",
        "debugging"
      ]
    },
    {
      "id": 10,
      "question": "What is boundary value analysis in testing?",
      "answer": "Boundary value analysis: Testing at edges of input ranges. Test: minimum, maximum, just below/above boundaries. Example: age 0-100, test: -1, 0, 1, 99, 100, 101. Most bugs occur at boundaries. More effective than random values. Part of equivalence partitioning technique.",
      "options": [
        "Testing geographic boundaries",
        "Testing at edges of input ranges (min/max)",
        "Testing only maximum values"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "techniques",
        "test-design"
      ]
    },
    {
      "id": 11,
      "question": "What is smoke testing and when should it be performed?",
      "answer": "Smoke testing: Quick shallow tests to verify basic functionality. When: After each build, before deeper testing. Purpose: Ensure build is stable enough for further testing. Example: Can app launch? Can user login? 15-30 min tests. Also called 'build verification testing'. Saves time by catching critical issues early.",
      "options": [
        "Testing in smoky environments",
        "Quick shallow tests after each build to verify stability",
        "Testing fire alarm systems"
      ],
      "correctOption": 1,
      "difficulty": "easy",
      "tags": [
        "smoke-testing",
        "build-verification"
      ]
    },
    {
      "id": 12,
      "question": "Explain equivalence partitioning technique.",
      "answer": "Equivalence partitioning: Dividing inputs into groups (partitions) that should behave similarly. Test one value from each partition. Example: age input - valid (18-65), too young (<18), too old (>65). Reduces test cases while maintaining coverage. Efficient test design technique.",
      "options": [
        "Dividing team members equally",
        "Grouping similar inputs and testing one per group",
        "Creating identical test cases"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "test-design",
        "equivalence-partitioning"
      ]
    },
    {
      "id": 13,
      "question": "What is the purpose of a test plan?",
      "answer": "Test plan documents testing strategy and approach. Includes: 1) Scope (what to test), 2) Resources (who, tools), 3) Schedule, 4) Test approach/strategy, 5) Entry/exit criteria, 6) Risks. Benefits: Alignment, planning, communication. Living document updated throughout project.",
      "options": [
        "A document listing all test cases",
        "Strategy document with scope, approach, resources, schedule",
        "Emergency backup plan for testing"
      ],
      "correctOption": 1,
      "difficulty": "easy",
      "tags": [
        "test-planning",
        "documentation"
      ]
    },
    {
      "id": 14,
      "question": "What is exploratory testing and when is it valuable?",
      "answer": "Exploratory testing: Simultaneous learning, test design, and execution. No predefined scripts. Tester explores like end user. Valuable for: 1) Finding unexpected bugs, 2) New features, 3) Time constraints, 4) Supplementing scripted tests. Requires experienced testers. Charter-based sessions for structure.",
      "options": [
        "Testing in different countries",
        "Simultaneous learning and testing without scripts",
        "Testing new exploration features"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "exploratory-testing",
        "manual-testing"
      ]
    },
    {
      "id": 15,
      "question": "What is the difference between verification and validation?",
      "answer": "Verification: Are we building the product right? (Meets specifications). Methods: Reviews, inspections, static testing. Validation: Are we building the right product? (Meets user needs). Methods: Testing, UAT. Example: Verification checks code follows design, validation checks design solves user problem.",
      "options": [
        "They mean the same thing",
        "Verification = built right, Validation = right product",
        "Verification is automated, validation is manual"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "verification",
        "validation"
      ]
    },
    {
      "id": 16,
      "question": "What is shift-left testing?",
      "answer": "Shift-left: Moving testing earlier in SDLC. Start testing in requirements/design phase, not just after coding. Benefits: 1) Find bugs earlier (cheaper to fix), 2) Better quality, 3) Faster delivery. Practices: Unit testing, TDD, early automation, involving QA in planning. Prevention over detection.",
      "options": [
        "Testing on left side of screen",
        "Moving testing earlier in development lifecycle",
        "Testing with left-handed developers"
      ],
      "correctOption": 1,
      "difficulty": "hard",
      "tags": [
        "shift-left",
        "strategy"
      ]
    },
    {
      "id": 17,
      "question": "What is continuous testing in DevOps/CI/CD?",
      "answer": "Continuous testing: Automated tests executed throughout CI/CD pipeline. Every code commit triggers tests. Includes: Unit, integration, E2E, performance tests. Benefits: 1) Fast feedback, 2) Catch issues early, 3) Safe deployments, 4) Quality gates. Integrated with tools like Jenkins, GitLab CI, GitHub Actions.",
      "options": [
        "Testing 24/7 manually",
        "Automated tests in CI/CD pipeline for every commit",
        "Never stopping test execution"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "continuous-testing",
        "CI-CD",
        "DevOps"
      ]
    },
    {
      "id": 18,
      "question": "What is mutation testing?",
      "answer": "Mutation testing: Testing the tests. Introduce small code changes (mutations) - if tests still pass, they're inadequate. Example: Change + to -, if tests pass, they're not checking correctly. Measures test effectiveness, not just coverage. Tools: Stryker, PIT. Computationally expensive but powerful.",
      "options": [
        "Testing genetic algorithms",
        "Testing tests by introducing code mutations",
        "Testing database migrations"
      ],
      "correctOption": 1,
      "difficulty": "hard",
      "tags": [
        "mutation-testing",
        "test-quality"
      ]
    },
    {
      "id": 19,
      "question": "What is the purpose of acceptance testing (UAT)?",
      "answer": "UAT (User Acceptance Testing): Final testing by actual users/stakeholders. Validates: Does it meet business requirements? Is it usable? When: After system testing, before production. Goal: Get user approval for release. Different from QA testing - focuses on business value, not technical correctness.",
      "options": [
        "Testing user login functionality",
        "Final testing by users to validate business requirements",
        "Testing all features are accepted"
      ],
      "correctOption": 1,
      "difficulty": "easy",
      "tags": [
        "UAT",
        "acceptance-testing"
      ]
    },
    {
      "id": 20,
      "question": "What is the difference between sanity and smoke testing?",
      "answer": "SMOKE: Broad & shallow, tests major functions, after every build, determines if build is testable. SANITY: Narrow & deep, tests specific functionality after bug fixes/changes, subset of regression. Example: Smoke checks app launches; Sanity verifies login bug fix works. Both are quick, subset tests.",
      "options": [
        "They are the same thing",
        "Smoke is broad after builds, Sanity is narrow after fixes",
        "Sanity is automated, smoke is manual"
      ],
      "correctOption": 1,
      "difficulty": "medium",
      "tags": [
        "smoke-testing",
        "sanity-testing"
      ]
    }
  ]
}
